{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 3: The Bayes Classifier\n",
    "## Associated Reading: Bishop 4.2 (Probabilistic Generative Models}\n",
    "\n",
    "# 1 The Lobster Dataset\n",
    "Let's imagine that you were a wildlife biologist studying lobsters, and you wanted to determine whether a given lobster would live or die.  What would you do?  The obvious answer would be to collect some data and see if there was a pattern of measurable variables with respect to which lobsters survived.  Fortunately, we have just such a dataset!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt('datasets/lobster_survive.dat',skiprows=1)\n",
    "data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains two elements: the first column is the lobster length, and the second column is whether that lobster survived or not (a one indicates survival).\n",
    "\n",
    "# IC3A \n",
    "**Fit a sensible model (-cough-Normal distribution-cough-) to each of the two populations (surviving and not-surviving lobsters) using the methods we developed in the previous topic**\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{m-1}\\sum (y_i - \\mu)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_died = data[data[:,1]==0]\n",
    "data_survived = data[data[:,1]==1]\n",
    "\n",
    "mu_died = np.mean(data_died[:,0])\n",
    "std_died = np.std(data_died[:,0])\n",
    "\n",
    "mu_survived = np.mean(data_survived[:,0])\n",
    "std_survived = np.std(data_survived[:,0])\n",
    "\n",
    "print(mu_died,std_died)\n",
    "print(mu_survived,std_survived)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 The Bayes Classifier\n",
    "Now, we'd like to use this information to determine whether a *new* lobster is more likely to die or survive.  We'll call each of these mutually exclusive outcomes a *class*: our goal is to *classify* new lobsters.  Note that the problem of *classification* is nominally different from the problem of *regression*.  In regression, we sought to predict something real-valued.  In classification, we are trying to predict something categorical (boolean or integer).  Otherwise, our previous framework still applies in the sense that we will have a set of *features* that we will run through a *model* in order to produce a *prediction*.  We will use *data* to adjust some *parameters* of the model such that our predictions are as close to observations as possible.  **What are the features and predictions in the lobster model problem**?\n",
    "\n",
    "We already have a fitted probability distribution for each of our populations, and we'd like to use that information to create a classifier.  We can do this easily using Bayes' Theorem.  \n",
    "\n",
    "One thing we might like to know is the probability of lobster survival given its length:\n",
    "$$\n",
    "P(\\text{Survive}=1|\\text{Length}=x),\n",
    "$$\n",
    "where $\\text{Survive}$ is a variable indicating whether the lobster survived or not, and $\\text{Length}$ is the length of the lobster ($x\\in\\mathcal{R}$).  Now we can use Bayes theorem to compute this probability directly\n",
    "$$\n",
    "P(\\text{Survive}=1|\\text{Length}=x) = \\frac{P(\\text{Length}=x|\\text{Survive}=1)P(\\text{Survive}=1)}{P(\\text{Length}=x)}.\n",
    "$$\n",
    "**What is the likelihood in this model, and can you compute it?  What is the prior in this model, and can you compute it?  How can you use this model to classify an example into the classes Survive=1 and Survive=0 (the answer to this latter question is almost trivially simple)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "mu_0 = np.mean(x[y==0])\n",
    "mu_1 = np.mean(x[y==1])\n",
    "sigma_0 = np.std(x[y==0])**2\n",
    "sigma_1 = np.std(x[y==1])**2\n",
    "prior = y.mean() # compute theta of Bernoulli\n",
    "\n",
    "def likelihood(x,mu,sigma2):\n",
    "    return 1./np.sqrt(2*np.pi*sigma2)*np.exp(-(x-mu)**2/(2*sigma2))\n",
    "\n",
    "evidence = likelihood(x,mu_1,sigma_1)*prior + likelihood(x,mu_0,sigma_0)*(1-prior)\n",
    "post = likelihood(x,mu_1,sigma_1)*prior/evidence\n",
    "\n",
    "bins = np.unique(x)\n",
    "plt.hist(x[y==0],bins,histtype='step',density=True,label='Died')\n",
    "plt.hist(x[y==1],bins,histtype='step',density=True,label='Survived')\n",
    "plt.legend()\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('P(Length)')\n",
    "ax = plt.twinx()\n",
    "ax.plot(x,post,'k-',label='Probability of Survival')\n",
    "ax.set_ylabel('Probability of survival')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common next step is to apply a *threshold* in order to explicitly classify new lobsters into discrete classes, but this is just throwing away information: it's typically more interesting to know the relative probability of classes, so that you know for which cases there remains significant uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Classes, multiple features\n",
    "\n",
    "All of our examples (both regression and classification) have been *univariate*: one feature on which to make predictions.  In a parallel way, the examples that we've looked at (binary classification and regression) are two endmembers of a spectrum: in binary classification, the prediction can be either positive or negative, while in regression the prediction can be any real number.  In practice, we'll want to relax all of these assumptions, so that we can use multiple features to classify objects into one of perhaps many classes.  Here's a dataset that will be useful to motivate this generalization:\n",
    "\n",
    "Imagine that one day you’re happily working away at your job as a research florist, and your boss frantically\n",
    "says to you: \"I just got a call from the board of regents, who say that they’re willing to fully fund the floristry\n",
    "department under one condition: that we can identify this species of iris that they’ve just sent us.\" Fortunately, we know a few things.  First, we've been informed that the species is definitely one of the following three species:  Iris setosa, Iris versicolor, or Iris virginica.  \n",
    "<img src=\"images/iris_setosa.jpg\" style=\"width: 400px;\">\n",
    "\n",
    "However, these irises look very similar, so we can’t tell just by looking. What should we do (with our department's financial solvency at stake)?  The reasonable answer is that you’re going to extract some data from\n",
    "the iris, and compare it to data from a bunch of other irises with known species. And it just so happens that\n",
    "we have access to a really nice database of flower metrics compiled by the very famous statistician [Ronald\n",
    "Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher).  The dataset has four attributes: petal length, petal width, sepal length, and sepal width. (By the way [sepals](https://en.wikipedia.org/wiki/Sepal) are the leaves that surround the flower. Let’s take a look at what these features look like.  Because this dataset is so famous and useful as a benchmark, it's available in python's scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets  # Import the sklearn datasets module\n",
    "import matplotlib as mpl      # Plotting tools\n",
    "mpl.rcParams['figure.figsize'] = [18,15]\n",
    "mpl.rcParams['font.size'] = 18\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data \n",
    "y = iris.target\n",
    "\n",
    "#Numeric class identifiers: 0 -> Setosa, 1-> Versicolor, 2-> Virginica\n",
    "classes = [0,1,2] \n",
    "\n",
    "N = len(classes) # The number of classes\n",
    "m = x.shape[0]   # The number of data points\n",
    "n = x.shape[1]   # The number of features\n",
    "\n",
    "# 2D plots of all possible permutations of 2 features out of 4 produces 4 choose 2 = 3 plots\n",
    "fig,axs = plt.subplots(nrows=4,ncols=4)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if i>j:\n",
    "            axs[i,j].scatter(x[:,i],x[:,j],c=y)\n",
    "            axs[i,j].set_xlabel(iris['feature_names'][i])\n",
    "            axs[i,j].set_ylabel(iris['feature_names'][j])\n",
    "        else: # delete redundant plots\n",
    "            fig.delaxes(axs[i,j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "784*784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to take a new set of iris measurements and output a class (an iris species).  We can use a similar procedure as the one we used above to make a classification.  Indeed the math looks pretty similar\n",
    "$$\n",
    "P(\\text{Species}=y|\\mathbf{X}=\\mathbf{x}) = \\frac{P(\\mathbf{X}=\\mathbf{x}|\\text{Species}=y) P(\\text{Species}=y)}{P(\\mathbf{X}=\\mathbf{x})}.\n",
    "$$\n",
    "For brevity, I'm going to write this as \n",
    "$$\n",
    "P(Y=y|\\mathbf{X}=\\mathbf{x}) = \\frac{P(\\mathbf{X}=\\mathbf{x}|Y=y)P(Y=y)}{P(\\mathbf{X}=\\mathbf{x})},\n",
    "$$\n",
    "where $Y$ is the species and $\\mathbf{X}$ is the data (sepal length, sepal width, etc.).  Once again, we have a prior that quantifies the probability that a new example is of a given species *prior* to looking at its features, a *likelihood* that tells us how likely it is that we would have observed the features that we did if the iris is of a given species, and the evidence which ensures that all the probabilities of the different classes add up to 1. \n",
    "\n",
    "There are a few differences though.  First is in how we want to model the prior $P(Y=y)$.  Before, we were able to use the Bernoulli distribution which gives the probability of successes, with a single parameter $\\theta$.  However, we can't use that here.  **Why not?**  \n",
    "\n",
    "Instead, we will use a very closely related distribution called the *Categorical distribution*, which is simply\n",
    "$$\n",
    "P(Y=y|\\boldsymbol{\\theta}) = \\prod_{k=1}^N \\theta_k^{[y=C_k]}\n",
    "$$\n",
    "where $C_k$ is the $k-th$ class in a set of $N$ possibilities (e.g. $C_k\\in[\\text{Virginica},\\text{Versicolor},\\text{Setosa}]$) and$\\theta_k$ is the probability of that class.  where $[y=C_k]$ is called the Iverson bracket, and it just means 1 if the statement within is true, zero if false. Because we assume that the species *has* to be one of these possibilites\n",
    "$$\n",
    "\\sum_{k=1}^N \\theta_k = 1.\n",
    "$$\n",
    "At first glance, this looks a bit different from the Bernoulli distribution, but its actually just a generalization.  To see this, let's set $N=2$ (a binary problem), which gives\n",
    "$$\n",
    "P(Y=y|\\theta_1,\\theta_2) = \\theta_1^{[y=1]} \\theta_2^{[y=0]},\n",
    "$$\n",
    "Now, if we use the constraint that $\\theta_2 = 1-\\theta_1$\n",
    "$$\n",
    "P(Y=y|\\theta_1) = \\theta_1^{y} (1-\\theta)^{1-y},\n",
    "$$\n",
    "which is just the Bernoulli distribution we saw before.  For a uniform prior on $\\theta$, we can compute the MAP value for $\\boldsymbol{\\theta}$ easily, using the same methods that we did for the Bernoulli and normal distribution, and we get\n",
    "$$\n",
    "\\theta_k = \\frac{\\sum_{i=1}^m [y=C_k]}{m},\n",
    "$$\n",
    "or \"the number of training data that were of a given species, divided by the total number of training data\".\n",
    "\n",
    "**Fit a categorical distribution to the iris dataset (Note especially that this has nothing to do with the features yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_vec = np.array([sum(y==i)/len(y) for i in range(N)])\n",
    "print(theta_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second change we need to make is to adapt our machinery to allow for multiple features.  The features in this case show up in the *likelihood*\n",
    "$$\n",
    "P(\\mathbf{X}=\\mathbf{x}|Y=C_k).\n",
    "$$\n",
    "$\\mathbf{x}$ is now a vector, and that means that the likelihood is a now a multidimensional distribution.  **In the case of the iris dataset, what is its dimensionality?** \n",
    "\n",
    "One way of dealing with this is to use a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution), which is given by\n",
    "$$\n",
    "P(\\mathbf{X}=\\mathbf{x}|\\boldsymbol{\\mu},\\Sigma) = (2\\pi)^{-\\frac{k}{2}} \\text{det}(\\Sigma)^{-\\frac{1}{2}}\\exp \\left[-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right],\n",
    "$$\n",
    "where $\\boldsymbol{\\mu}$ is a *vector* of means (one for each dimension in the data), and $\\Sigma$ is a matrix called the *covariance*.  Despite it looking a little bit scary, this thing actually isn't so bad: it just generalizes the bell curve to two dimensions, so that instead of normally distributed variables looking like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.random.randn(1000),20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In two dimensions, it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = np.array([[1,0.0],\n",
    "                  [0.0,1]])\n",
    "A = np.linalg.cholesky(Sigma)\n",
    "X = A @ np.random.randn(2,10000) \n",
    "X = X.T\n",
    "plt.plot(X[:,0],X[:,1],'k.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where if we took a in either dimension, it would look like a bell curve.\n",
    "\n",
    "Nonetheless, using this full distribution is challenging because of $\\Sigma$: it's a (symmetric) matrix in $\\mathcal{R}^{n\\times n}$, and as such has $\\mathcal{O}(n^2)$ parameters that we would need to infer.  As the number of features in our dataset grows large (as it will for, say, image data), this will become prohibitive.  \n",
    "\n",
    "In order to keep the number of parameters reasonable, we will use the so-called *naive* assumption, which states that all of the features are *independent*.  In the case of the iris, this means that we assume that, for example, the sepal width and sepal length are uncorrelated.  **Looking back at these data, does this seem like a good assumption?**\n",
    "\n",
    "Regardless of whether its valid or not, it is very helpful to be able to say that\n",
    "$$\n",
    "P(\\mathbf{X}=\\mathbf{x}|Y=C_k) \\approx \\prod_{j=1}^N P(X_j=x_j|Y=C_k).\n",
    "$$\n",
    "If $P(X_j=x_j|Y=C_k)$ are univariate normal distributions, then now there is simply two parameters for each feature for each class: one mean and one variance. \n",
    "\n",
    "These end up being straightforward to compute:\n",
    "$$\n",
    "\\mu_{jk} = \\frac{\\sum_{i=1}^m x_{ij} [y_i=C_k]}{\\sum_{i=1}^m [y_i=C_k]},\n",
    "$$\n",
    "$$\n",
    "\\sigma^2_{jk} = \\frac{\\sum_{i=1}^m (x_{ij} - \\mu_{jk}) [y_i=C_k]}{\\sum_{i=1}^m [y_i=C_k]},\n",
    "$$\n",
    "where the Iverson bracket has been used again.  To be clear, $j$ indexes the feature of interest (petal length, sepal length, etc.) while $k$ indexes the class (virginica, setosa, etc.).  Put in words, the above formulas state: take your training data, split it by observed class and by the feature that you're interested in.  Then, take the sample mean and sample variance of each of these splits. \n",
    "\n",
    "To be clear, what we want to do is to fit a normal distribution to *each* histogram in each of these four plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for each class\n",
    "colors = [[0.267004, 0.004874, 0.329415, 1.      ],\n",
    "          [0.127568, 0.566949, 0.550556, 1.      ],\n",
    "          [0.993248, 0.906157, 0.143936, 1.      ]]\n",
    "names = ['Setosa','Versicolor','Virginica']\n",
    "\n",
    "x_new = np.array([7.65, 3.3 , 6.25, 1.95])\n",
    "\n",
    "# Plot histograms of all the features\n",
    "fig,axs = plt.subplots(nrows=2,ncols=2)\n",
    "fig.subplots_adjust(hspace=0.3,wspace=0.3)\n",
    "for i in range(4):\n",
    "    p = i%2\n",
    "    q = i//2\n",
    "    axs[p,q].hist(x[y==0,i],histtype='step',color=colors[0],label=names[0])\n",
    "    axs[p,q].hist(x[y==1,i],histtype='step',color=colors[1],label=names[1])\n",
    "    axs[p,q].hist(x[y==2,i],histtype='step',color=colors[2],label=names[2])\n",
    "    axs[p,q].axvline(x_new[i], color='red')\n",
    "    axs[p,q].set_xlabel(iris.feature_names[i])\n",
    "    axs[p,q].set_ylabel('Count')\n",
    "axs[0,0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a new example (the red line), we'll ask: \"which of these classes are the features of this new example consistent with?\" We'll multiply these with each other then by the prior likelihood of each of these classes (are some irises extremely rare?), and this will tell us the relative probability of each iris species.  \n",
    "\n",
    "## IC3B Fitting irises\n",
    "With this material in place, you are now ready to train and test a naive Bayes classifier on the iris dataset.  The first step is to compute and store the values of $\\theta$ for each class, and the values of $\\mu$ and $\\sigma^2$ for each feature and each class.  Do this for a randomly selected training set (already split for you below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as ms\n",
    "x_train,x_test,y_train,y_test = ms.train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([sum(y_train==i)/len(y_train) for i in range(3)])\n",
    "mus = np.array([x_train[y_train==i].mean(axis=0) for i in range(3)])\n",
    "sigmas = np.array([x_train[y_train==i].std(axis=0) for i in range(3)])\n",
    "print(mus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute the posterior probability of each class for each test set example.  You can simplify this by ignoring the *evidence* (the denominator in Bayes theorem), and simply classifying based on which of the resulting *unscaled* posterior probabilities is greatest.  **Why is it okay to ignore the evidence in this case?**  Make a scatter plot with both the training and test data, colored by iris species but with different symbols to indicate whether a given example comes from the test or training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to classify \n",
    "\n",
    "#P(Y=C_0|X=x) = P(X=x|Y=C_0) P(Y=C_0) / evidence\n",
    "# P(X=x|Y=C_0) = prod P(X_i=x_i | C_k)\n",
    "\n",
    "x_new = x_test[0]\n",
    "\n",
    "def normal(x,mu,sigma):\n",
    "    return 1/(np.sqrt(2*np.pi)*sigma)*np.exp(-0.5*(x-mu)**2/sigma**2)\n",
    "\n",
    "def unscaled_posterior(x):\n",
    "    posteriors = np.array([1.,1,1])\n",
    "    for k in range(N):   \n",
    "        prior = theta[k]\n",
    "        likelihood = 1\n",
    "        for j in range(n):\n",
    "            likelihood *= normal(x_new[j],mus[k,j],sigmas[k,j])\n",
    "        posteriors[k] *= prior*likelihood\n",
    "    return posteriors\n",
    "\n",
    "unscaled_prob = unscaled_posterior(x_new)\n",
    "evidence = unscaled_prob.sum()\n",
    "posteriors = unscaled_prob/evidence\n",
    "\n",
    "print(posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
